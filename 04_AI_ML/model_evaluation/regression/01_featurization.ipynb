{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96994813-a0a9-4507-8931-1a696bf27863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the databricks-feature-engineering package\n",
    "try:\n",
    "    import databricks.feature_engineering\n",
    "    print(\"Package already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Installing package...\")\n",
    "    %pip install databricks-feature-engineering\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36a65ad5-9616-4e8d-8ad9-3654949591ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "# Note: Use ML-supported clusters (labeled as \"X.Y LTS ML\" e.g., \"13.3 LTS ML\") for machine learning workloads.\n",
    "# Import required libraries\n",
    "from pyspark.sql.functions import current_date, year, col\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from databricks.feature_engineering import FeatureEngineeringClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15dd42d0-d59c-4894-8f79-1102689b97ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "# Load housing dataset from CSV"
    }
   },
   "outputs": [],
   "source": [
    "# Load Housing Data from CSV\n",
    "base_file_path = \"file:/Workspace/Users/yasodhashree91@gmail.com/oms-databricks/04_AI_ML/model_evaluation/regression/data_files/\"\n",
    "housing_df = spark.read.csv(base_file_path + \"housing.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display first 5 rows for understanding\n",
    "print(\"Housing Data Preview:\")\n",
    "housing_df.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b658403a-e956-489f-b01c-46a842869208",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop rows with any null values"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the target label column before handling missing values in the features\n",
    "features_df = housing_df.drop(\"Price\")\n",
    "\n",
    "# Drop rows with any null values in the features\n",
    "features_df = features_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad66a8d-4b6e-401a-84c2-c28fe40a73ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create House_Age feature"
    }
   },
   "outputs": [],
   "source": [
    "# Create House_Age feature based on Year_Built\n",
    "housing_df = housing_df.withColumn(\"House_Age\", year(current_date()) - col(\"Year_Built\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa85272-7d69-4d0e-8032-8de235d376e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scale numeric features"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Columns to scale\n",
    "numeric_cols = [\"Square_Feet\", \"Num_Bedrooms\", \"Num_Bathrooms\", \"Num_Floors\", \n",
    "                \"Garage_Size\", \"Location_Score\", \"Distance_to_Center\", \"House_Age\"]\n",
    "\n",
    "# Cast all numeric columns to float\n",
    "for c in numeric_cols:\n",
    "    housing_df = housing_df.withColumn(c, col(c).cast(\"float\"))\n",
    "\n",
    "# Step 1: Assemble into vector\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features_vector\")\n",
    "housing_df = assembler.transform(housing_df)\n",
    "\n",
    "# Step 2: Apply StandardScaler directly\n",
    "scaler = StandardScaler(inputCol=\"features_vector\", outputCol=\"scaled_vector\", withStd=True, withMean=False)\n",
    "scaler_model = scaler.fit(housing_df)\n",
    "housing_df_scaled = scaler_model.transform(housing_df)\n",
    "\n",
    "# Step 3: Convert vector to array\n",
    "housing_df_scaled = housing_df_scaled.withColumn(\"scaled_array\", vector_to_array(\"scaled_vector\"))\n",
    "\n",
    "# Step 4: Create individual scaled columns\n",
    "for i, c in enumerate(numeric_cols):\n",
    "    housing_df_scaled = housing_df_scaled.withColumn(f\"scaled_{c}\", col(\"scaled_array\")[i])\n",
    "\n",
    "# Display final result\n",
    "display(housing_df_scaled.select(\"House_ID\", *[f\"scaled_{c}\" for c in numeric_cols]).limit(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c98f352-2db4-4eac-ab64-30fe073e1b58",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Select only required columns (drop others)"
    }
   },
   "outputs": [],
   "source": [
    "# Select required columns for feature store\n",
    "final_cols = [\"House_ID\", \"Has_Garden\", \"Has_Pool\"] + [f\"scaled_{col}\" for col in numeric_cols]\n",
    "\n",
    "housing_features_final = housing_df_scaled.select(final_cols)\n",
    "\n",
    "# Display final feature table\n",
    "print(\"Final Features for Feature Store:\")\n",
    "housing_features_final.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8adc9a15-0854-4b64-b9e8-711d2bf17825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save Features to Databricks Feature Store\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS realestate.ml\")\n",
    "\n",
    "feature_table_name = \"realestate.ml.housing_features\"\n",
    "\n",
    "# Drop existing table (if any)\n",
    "try:\n",
    "    fe.drop_table(name=feature_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create feature table\n",
    "fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[\"House_ID\"],\n",
    "    df=housing_features_final,\n",
    "    description=\"Housing features for price prediction\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba61ec2-8da2-49c5-94bb-ce6f1c267388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"realestate.ml.housing_features\").limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_featurization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
