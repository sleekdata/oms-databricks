{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057d32b0-755d-4bdd-a12d-9848b068fb43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the databricks-feature-engineering package\n",
    "try:\n",
    "    import databricks.feature_engineering\n",
    "    print(\"Package already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Installing package...\")\n",
    "    %pip install databricks-feature-engineering\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5bbabf-c069-4705-99fe-9621626d4b9e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "# Imports"
    }
   },
   "outputs": [],
   "source": [
    "# Note: Use ML-supported clusters (labeled as \"X.Y LTS ML\" e.g., \"13.3 LTS ML\") for machine learning workloads.\n",
    "\n",
    "from pyspark.sql.functions import avg, sum, count, when, current_date, datediff, col, split, regexp_replace, expr\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from databricks.feature_engineering import FeatureEngineeringClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7239b8-9ed9-43a0-9a2a-0bdb41026530",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Data from CSV Files"
    }
   },
   "outputs": [],
   "source": [
    "# Load customers, transactions, and interactions data from CSV files (CSV used here for demo; typically, data comes from tables)\n",
    "base_file_path = \"file:/Workspace/Users/yasodhashree91@gmail.com/oms-databricks/04_AI_ML/feature_eng/data_files/\"\n",
    "customers_df = spark.read.csv(base_file_path + \"customers.csv\", header=True)\n",
    "transactions_df = spark.read.csv(base_file_path + \"transactions.csv\", header=True)\n",
    "interactions_df = spark.read.csv(base_file_path + \"interactions.csv\", header=True)\n",
    "\n",
    "\n",
    "# Display the first few rows of each DataFrame for understanding\n",
    "print(\"Customers Data:\")\n",
    "customers_df.limit(5).display()\n",
    "\n",
    "print(\"Transactions Data:\")\n",
    "transactions_df.limit(5).display()\n",
    "\n",
    "print(\"Interactions Data:\")\n",
    "interactions_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec768bde-e4f7-42a4-8100-b0de7f1a090a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean Columns for Better Compatibility"
    }
   },
   "outputs": [],
   "source": [
    "# Function to rename columns to be consistent and compatible with various ML algorithms and Databricks Feature Store\n",
    "# Define Function\n",
    "def renameColumns(df):\n",
    "    renamed_df = df\n",
    "    for column in df.columns:\n",
    "        clean_name = column.replace(' ', '_').replace('/', '_').replace('-', '_')\n",
    "        renamed_df = renamed_df.withColumnRenamed(column, clean_name)\n",
    "    return renamed_df\n",
    "\n",
    "\n",
    "# Run function for each soruce dataframe\n",
    "customers_df = renameColumns(customers_df)\n",
    "transactions_df = renameColumns(transactions_df)\n",
    "interactions_df = renameColumns(interactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e33ac9-9a60-4960-ada9-b7d23a142859",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Handle NULL/Missing Values"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fill missing or NULL values with the average value calculated from all remaining customers\n",
    "avg_age = int(customers_df.select(avg(\"age\")).first()[0])\n",
    "avg_score = int(customers_df.select(avg(\"credit_score\")).first()[0])\n",
    "\n",
    "customers_df = customers_df.fillna({\n",
    "    \"age\": avg_age,\n",
    "    \"credit_score\": avg_score\n",
    "})\n",
    "\n",
    "# Skip rows when important fields like income range are missing\n",
    "customers_df = customers_df.filter(customers_df[\"income_range\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2850d38d-e312-4024-9e5f-ea3a2a420c6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregate and Join Transaction Features"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate transaction data (total spent and transaction count) per customer and join with customers dataframe\n",
    "\n",
    "txn_agg = transactions_df.groupBy(\"customer_id\").agg(\n",
    "    sum(\"amount\").alias(\"total_spent\"),\n",
    "    count(\"transaction_id\").alias(\"txn_count\")\n",
    ")\n",
    "customers_features = customers_df.join(txn_agg, on=\"customer_id\", how=\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0740b1b-f968-4fe1-929f-33357e18d2af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregate and Join Interaction Features"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate interaction data (average satisfaction and complaint count) per customer and join with customer dataframe\n",
    "\n",
    "interaction_agg = interactions_df.groupBy(\"customer_id\").agg(\n",
    "    avg(\"satisfaction_score\").alias(\"avg_satisfaction\"),\n",
    "    sum(when(interactions_df.reason == \"Complaint\", 1).otherwise(0)).alias(\"complaint_count\")\n",
    ")\n",
    "\n",
    "customers_features = customers_features.join(interaction_agg, on=\"customer_id\", how=\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd1f38c-69d2-4cdd-83b0-2177deff4697",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert income range string to average numeric value"
    }
   },
   "outputs": [],
   "source": [
    "# Create feature income_avg based on income_range\n",
    "# Example: \"2000-4000\" becomes 3000 ((2000 + 4000) / 2)\n",
    "\n",
    "customers_features = customers_features.withColumn(\n",
    "    \"income_avg\",\n",
    "    (\n",
    "        expr(\"cast(regexp_replace(split(income_range, '-')[0], ',', '') as double)\") +\n",
    "        expr(\"cast(regexp_replace(split(income_range, '-')[1], ',', '') as double)\")\n",
    "    ) / 2\n",
    ").drop(\"income_range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d19b764f-8845-4d02-8597-e656981a65f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert Join Date to Customer Tenure (Days Since Joining)"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate customer_tenure as days from join_date till today\n",
    "customers_features = customers_features.withColumn(\n",
    "    \"customer_tenure\",\n",
    "    datediff(current_date(), col(\"join_date\"))\n",
    ").drop(\"join_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d4956e-d34b-44a5-bd5f-7b392b343076",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Drop Columns That Do Not Affect Prediction"
    }
   },
   "outputs": [],
   "source": [
    "# Drop columns that are non-signals for the prediction we are making\n",
    "drop_cols = [\"first_name\", \"last_name\", \"email\"]\n",
    "customers_features = customers_features.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd9e728-8681-4e35-8654-30f8aa59e726",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Encoding for Categorical Features"
    }
   },
   "outputs": [],
   "source": [
    "# Ordinal Encoding - When categories have natural order \n",
    "# Example: card_tier (Basic < Silver < Gold < Platinum)\n",
    "\n",
    "\n",
    "customers_features = customers_features.withColumn(\n",
    "    'card_tier_encoded',\n",
    "    when(col('card_tier') == 'Basic', 0)\n",
    "    .when(col('card_tier') == 'Silver', 1)\n",
    "    .when(col('card_tier') == 'Gold', 2)\n",
    "    .when(col('card_tier') == 'Platinum', 3)\n",
    "    .otherwise(None)\n",
    "    .cast(\"double\")\n",
    ").drop(\"card_tier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27f558a3-0457-4797-b26d-1cb6c4d2244c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Encoding for Categorical Features"
    }
   },
   "outputs": [],
   "source": [
    "# One-Hot Encoding - When categories have no meaningful order\n",
    "# Example: gender (Male/Female/Other)\n",
    "\n",
    "customers_features = customers_features \\\n",
    "    .withColumn(\"gender_female\", when(col(\"gender\") == \"Female\", 1).otherwise(0).cast(\"double\")) \\\n",
    "    .withColumn(\"gender_male\", when(col(\"gender\") == \"Male\", 1).otherwise(0).cast(\"double\")) \\\n",
    "    .withColumn(\"gender_other\", when(col(\"gender\") == \"Other\", 1).otherwise(0).cast(\"double\")) \\\n",
    "    .drop(\"gender\")\n",
    "\n",
    "display(customers_features.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7783d6a0-211d-49d3-b3c5-95b3e904c5e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scale numerical features to avoid bias"
    }
   },
   "outputs": [],
   "source": [
    "# Scale numeric features like age (~0-100) and income (~10000s) to a similar scale to avoid ML giving more weight to larger-scale features (e.g., income)\n",
    "\n",
    "columns_to_scale = [\n",
    "    \"age\", \"credit_score\", \"income_avg\", \"customer_tenure\",\n",
    "    \"total_spent\", \"txn_count\", \"avg_satisfaction\", \"complaint_count\"\n",
    "]\n",
    "\n",
    "# Cast columns to float\n",
    "for c in columns_to_scale:\n",
    "    customers_features = customers_features.withColumn(c, col(c).cast(\"float\"))\n",
    "\n",
    "assembler = VectorAssembler(inputCols=columns_to_scale, outputCol=\"features_vector\")\n",
    "scaler = StandardScaler(inputCol=\"features_vector\", outputCol=\"scaled_vector\", withStd=True, withMean=False)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "model = pipeline.fit(customers_features)\n",
    "customers_features_scaled = model.transform(customers_features)\n",
    "\n",
    "# Convert vector column to array column using built-in function (faster, no udf)\n",
    "customers_features_scaled = customers_features_scaled.withColumn(\n",
    "    \"scaled_array\",\n",
    "    vector_to_array(\"scaled_vector\")\n",
    ")\n",
    "\n",
    "# Create individual scaled columns from array elements\n",
    "for i, c in enumerate(columns_to_scale):\n",
    "    customers_features_scaled = customers_features_scaled.withColumn(f\"scaled_{c}\", col(\"scaled_array\")[i])\n",
    "\n",
    "display(customers_features_scaled.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1904a581-be2b-4f23-b406-f15293d58439",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "After converting raw fields into features drop the raw columns"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only the final feature columns and exclude the target label (is_churn)\n",
    "final_cols = [\n",
    "    \"customer_id\",\n",
    "    \"card_tier_encoded\",\n",
    "    \"gender_female\", \"gender_male\", \"gender_other\"\n",
    "]\n",
    "\n",
    "# Add all the newly created scaled columns\n",
    "scaled_cols = [f\"scaled_{c}\" for c in columns_to_scale]\n",
    "\n",
    "# Select customer_id, categorical features, and all individual scaled feature columns\n",
    "customers_features_final = customers_features_scaled.select(final_cols + scaled_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dda0349-f4cd-49c4-b6ed-30381a436008",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Store the final DataFrame in the Databricks Feature Store."
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS oms_analytics.feature_store\")\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "feature_table_name = \"oms_analytics.feature_store.customers_features\"\n",
    "\n",
    "# Drop the feature table if it exists\n",
    "try:\n",
    "    fe.drop_table(name=feature_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create the feature table\n",
    "fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[\"customer_id\"],\n",
    "    df=customers_features_final,\n",
    "    description = \"\"\"\n",
    "    Preprocessed customer features for:\n",
    "    - Churn prediction (Identify customers who might leave) \n",
    "    - Lifetime value (LTV) modeling \n",
    "    - Customer segmentation\n",
    "\n",
    "    Includes: demographics, financials, engagement metrics, and satisfaction scores.\n",
    "    Primary key: customer_id (unique identifier)        \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfceeb43-62c1-42e9-9a6c-5335e93b6f2d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display and review the contents of the Feature Store"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"oms_analytics.feature_store.customers_features\").limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_featurization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
