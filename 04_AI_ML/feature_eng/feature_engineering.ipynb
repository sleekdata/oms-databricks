{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7239b8-9ed9-43a0-9a2a-0bdb41026530",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Data from CSV Files"
    }
   },
   "outputs": [],
   "source": [
    "# Define the base file path as a parameter\n",
    "base_file_path = \"file:/Workspace/Users/yasodhashree91@gmail.com/oms-databricks/04_AI_ML/feature_eng/data_files/\"\n",
    "\n",
    "# Load all data from CSV files\n",
    "customers_df = spark.read.csv(base_file_path + \"customers.csv\", header=True)\n",
    "transactions_df = spark.read.csv(base_file_path + \"transactions.csv\", header=True)\n",
    "interactions_df = spark.read.csv(base_file_path + \"interactions.csv\", header=True)\n",
    "\n",
    "# Optionally display the first 5 rows of each DataFrame\n",
    "print(\"Customers Data:\")\n",
    "customers_df.limit(5).display()\n",
    "\n",
    "print(\"Transactions Data:\")\n",
    "transactions_df.limit(5).display()\n",
    "\n",
    "print(\"Interactions Data:\")\n",
    "interactions_df.limit(5).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec768bde-e4f7-42a4-8100-b0de7f1a090a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean Columns for Better Compatibility"
    }
   },
   "outputs": [],
   "source": [
    "# Function to rename columns to be consistent and compatible with various ML algorithms and Databricks Feature Store\n",
    "def renameColumns(df):\n",
    "    renamed_df = df\n",
    "    for column in df.columns:\n",
    "        clean_name = column.replace(' ', '_').replace('/', '_').replace('-', '_')\n",
    "        renamed_df = renamed_df.withColumnRenamed(column, clean_name)\n",
    "    return renamed_df\n",
    "\n",
    "\n",
    "# Run function for each soruce dataframe\n",
    "customers_df = renameColumns(customers_df)\n",
    "transactions_df = renameColumns(transactions_df)\n",
    "interactions_df = renameColumns(interactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e33ac9-9a60-4960-ada9-b7d23a142859",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Handle NULL/Missing Values"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate the average values for age and credit_score to be used where they are missing\n",
    "avg_age = customers_df.select(avg(\"age\")).first()[0]\n",
    "avg_score = customers_df.select(avg(\"credit_score\")).first()[0]\n",
    "\n",
    "# Fill null values for age and credit_score with their respective averages\n",
    "customers_df = customers_df.fillna({\n",
    "    \"age\": avg_age,\n",
    "    \"credit_score\": avg_score\n",
    "})\n",
    "\n",
    "# Skip rows where income_range is null\n",
    "customers_df = customers_df.filter(customers_df[\"income_range\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2850d38d-e312-4024-9e5f-ea3a2a420c6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregate and Join Transaction Features"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, count\n",
    "\n",
    "txn_agg = transactions_df.groupBy(\"customer_id\").agg(\n",
    "    spark_sum(\"amount\").alias(\"total_spent\"),\n",
    "    count(\"transaction_id\").alias(\"txn_count\")\n",
    ")\n",
    "customers_features = customers_df.join(txn_agg, on=\"customer_id\", how=\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0740b1b-f968-4fe1-929f-33357e18d2af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Aggregate and Join Interaction Features"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum, when\n",
    "\n",
    "interaction_agg = interactions_df.groupBy(\"customer_id\").agg(\n",
    "    avg(\"satisfaction_score\").alias(\"avg_satisfaction\"),\n",
    "    sum(when(interactions_df.reason == \"Complaint\", 1).otherwise(0)).alias(\"complaint_count\")\n",
    ")\n",
    "\n",
    "customers_features_df = customers_features_df.join(interaction_agg, on=\"customer_id\", how=\"left\").fillna(0)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
