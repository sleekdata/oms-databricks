{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b8a090-b8e2-468f-981e-5ad9ff7ce41d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Read streaming data from the Silver tables\n",
    "orders_df = spark.readStream.table(\"oms_analytics.silver.orders\").alias(\"orders\")\n",
    "order_items_df = spark.readStream.table(\"oms_analytics.silver.order_items\").alias(\"order_items\")\n",
    "\n",
    "# Rename order timestamp to avoid ambiguity\n",
    "order_items_df = order_items_df.withColumnRenamed(\"order_timestamp\", \"order_item_timestamp\")\n",
    "\n",
    "# Add watermark to handle late arrivals\n",
    "orders_df = orders_df.withWatermark(\"order_timestamp\", \"1 minutes\")\n",
    "order_items_df = order_items_df.withWatermark(\"order_item_timestamp\", \"1 minutes\")\n",
    "\n",
    "# Join the orders and order_items dataframes\n",
    "joined_df = orders_df.join(order_items_df, \"order_id\")\n",
    "\n",
    "# Aggregate and add other required fields\n",
    "aggregated_df = joined_df \\\n",
    "    .groupBy(\n",
    "        \"date_id\",\n",
    "        \"customer_id\",\n",
    "        \"product_id\",\n",
    "        F.window(\"order_timestamp\", \"1 minutes\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        F.sum(\"quantity\").alias(\"items_sold\"),\n",
    "        F.sum(\"line_total\").alias(\"sales_amount\")\n",
    "    ) \\\n",
    "    .drop(\"window\") \n",
    "\n",
    "final_df = aggregated_df \\\n",
    "    .withColumn(\"surrogate_key\", F.sha2(F.concat_ws(\"_\", \n",
    "        F.col(\"date_id\").cast(\"string\"),\n",
    "        F.col(\"customer_id\").cast(\"string\"),\n",
    "        F.col(\"product_id\").cast(\"string\")\n",
    "    ), 256)) \\\n",
    "    .withColumn(\"process_id\", F.lit(\"de_nb_102\")) \\\n",
    "    .withColumn(\"gold_load_ts\", F.current_timestamp()) \\\n",
    "    .select(\n",
    "        \"surrogate_key\",\n",
    "        \"customer_id\",\n",
    "        \"date_id\",\n",
    "        \"product_id\",\n",
    "        \"items_sold\",\n",
    "        \"sales_amount\",\n",
    "        \"process_id\",\n",
    "        \"gold_load_ts\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the checkpoint locations\n",
    "external_location_name = \"abfss://orders@omslanding.dfs.core.windows.net\"\n",
    "checkpoint_location_daily_sales_fact = f\"{external_location_name}/checkpoints/gold_loader/daily_sales_fact\"\n",
    "\n",
    "# Define the function to upsert into Delta table\n",
    "def upsert_to_delta(final_df, batchId):\n",
    "    print(\"Batch ID:\", batchId)\n",
    "    print(\"Record Count:\", final_df.count())\n",
    "\n",
    "    if not spark.catalog.tableExists(\"oms_analytics.gold.daily_sales_fact\"):\n",
    "        final_df.write.format(\"delta\").saveAsTable(\"oms_analytics.gold.daily_sales_fact\")\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, \"oms_analytics.gold.daily_sales_fact\")\n",
    "        delta_table.alias(\"tgt\").merge(\n",
    "            final_df.alias(\"src\"),\n",
    "            \"tgt.customer_id = src.customer_id AND tgt.date_id = src.date_id AND tgt.product_id = src.product_id\"\n",
    "        ).whenMatchedUpdate(\n",
    "            set={\n",
    "                \"items_sold\": \"tgt.items_sold + src.items_sold\",\n",
    "                \"sales_amount\": \"tgt.sales_amount + src.sales_amount\",\n",
    "                \"gold_load_ts\": \"src.gold_load_ts\"\n",
    "            }\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={\n",
    "                \"surrogate_key\": \"src.surrogate_key\",\n",
    "                \"customer_id\": \"src.customer_id\",\n",
    "                \"date_id\": \"src.date_id\",\n",
    "                \"product_id\": \"src.product_id\",\n",
    "                \"items_sold\": \"src.items_sold\",\n",
    "                \"sales_amount\": \"src.sales_amount\",\n",
    "                \"process_id\": \"src.process_id\",\n",
    "                \"gold_load_ts\": \"src.gold_load_ts\"\n",
    "            }\n",
    "        ).execute()\n",
    "\n",
    "# Write the aggregated metrics to a Delta table in real-time using foreachBatch\n",
    "query = final_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(upsert_to_delta) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location_daily_sales_fact) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "daily_sales_fact_loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
