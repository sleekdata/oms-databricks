{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b246958a-97ab-4460-a22b-2e4382af420d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Read streaming data from the Silver tables\n",
    "orders_df = spark.readStream.table(\"oms_analytics.silver.orders\")\n",
    "order_items_df = spark.readStream.table(\"oms_analytics.silver.order_items\")\n",
    "\n",
    "# Rename to avoid ambiguity\n",
    "order_items_df = order_items_df.withColumnRenamed(\"order_timestamp\", \"order_item_timestamp\")\n",
    "\n",
    "# Add watermark to handle late data\n",
    "orders_df = orders_df.withWatermark(\"order_timestamp\", \"5 minutes\")\n",
    "order_items_df = order_items_df.withWatermark(\"order_item_timestamp\", \"5 minutes\")\n",
    "\n",
    "# Join the DataFrames on order_item_id\n",
    "joined_df = orders_df.join(order_items_df, on=\"order_id\", how=\"inner\")\n",
    "\n",
    "# Aggregate the data by date_id, customer_id, product_id using a time window\n",
    "daily_aggregates_df = joined_df.groupBy(\n",
    "    \"date_id\",   \n",
    "    \"customer_id\",\n",
    "    \"product_id\",\n",
    "    F.window(\"order_timestamp\", \"5 minutes\")\n",
    ").agg(\n",
    "    F.sum(\"quantity\").alias(\"items_sold\"),\n",
    "    F.sum(\"line_total\").alias(\"sales_amount\")\n",
    ").select(\n",
    "    \"date_id\",\n",
    "    \"customer_id\",\n",
    "    \"product_id\",\n",
    "    \"items_sold\",\n",
    "    \"sales_amount\"\n",
    ")\n",
    "\n",
    "# Add surrogate_key and additional columns\n",
    "daily_aggregates_df = daily_aggregates_df \\\n",
    "    .withColumn(\"surrogate_key\", F.concat_ws(\"_\", \n",
    "        F.col(\"date_id\").cast(\"string\"),\n",
    "        F.col(\"customer_id\").cast(\"string\"),\n",
    "        F.col(\"product_id\").cast(\"string\")\n",
    "    )) \\\n",
    "    .withColumn(\"process_id\", F.lit(\"de_nb_102\")) \\\n",
    "    .withColumn(\"gold_load_ts\", F.current_timestamp())\n",
    "\n",
    "\n",
    "# Define the external location for Azure Data Lake Storage\n",
    "external_location_name = \"abfss://orders@omslanding.dfs.core.windows.net\"\n",
    "checkpoint_location_daily_sales_fact = f\"{external_location_name}/checkpoints/gold_loader/daily_sales_fact\"\n",
    "\n",
    "# Create the table if it doesn't exist\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS oms_analytics.gold.daily_sales_fact (\n",
    "    surrogate_key STRING,\n",
    "    date_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "    items_sold DOUBLE,\n",
    "    sales_amount DOUBLE,\n",
    "    process_id STRING,\n",
    "    gold_load_ts TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Define the upsert logic as a separate function named upsert_data\n",
    "def upsert_data(batch_df, batch_id):\n",
    "    # Define the target Delta table\n",
    "    target_table = DeltaTable.forName(spark, \"oms_analytics.gold.daily_sales_fact\")\n",
    "    \n",
    "    # Define the merge condition\n",
    "    merge_condition = (\n",
    "        \"target.date_id = source.date_id AND \"\n",
    "        \"target.customer_id = source.customer_id AND \"\n",
    "        \"target.product_id = source.product_id\"\n",
    "    )\n",
    "    \n",
    "    # Perform the MERGE operation\n",
    "    target_table.alias(\"target\").merge(\n",
    "        batch_df.alias(\"source\"),\n",
    "        merge_condition\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(set={\n",
    "        \"items_sold\": \"source.items_sold\",\n",
    "        \"sales_amount\": \"source.sales_amount\"\n",
    "    }) \\\n",
    "    .whenNotMatchedInsert(values={\n",
    "        \"surrogate_key\": \"source.surrogate_key\",\n",
    "        \"date_id\": \"source.date_id\",\n",
    "        \"customer_id\": \"source.customer_id\",\n",
    "        \"product_id\": \"source.product_id\",\n",
    "        \"items_sold\": \"source.items_sold\",\n",
    "        \"sales_amount\": \"source.sales_amount\",\n",
    "        \"process_id\": \"source.process_id\",\n",
    "        \"gold_load_ts\": \"source.gold_load_ts\"\n",
    "    }) \\\n",
    "    .execute()\n",
    "\n",
    "# Write the streaming data to the table using foreachBatch\n",
    "writequery = daily_aggregates_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(upsert_data) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location_daily_sales_fact) \\\n",
    "    .start()\n",
    "\n",
    "# Start the streaming process\n",
    "writequery.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "backup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
